{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb7f620",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdba1cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#configuration\n",
    "DATA_DIR = \"dataset/asl_alphabet_train\"   # path to train folders (each folder is a class)\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 64\n",
    "SEED = 1337\n",
    "NUM_CLASSES = 29\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "EPOCHS_HEAD = 6\n",
    "EPOCHS_FINETUNE = 10\n",
    "PATIENCE = 3\n",
    "MODEL_DIR = \"models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bcb378",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#utils\n",
    "def get_label_names(data_dir):\n",
    "    # alphabetical sorted class folder names\n",
    "    classes = sorted([p.name for p in pathlib.Path(data_dir).iterdir() if p.is_dir()])\n",
    "    return classes\n",
    "\n",
    "CLASS_NAMES = get_label_names(DATA_DIR)\n",
    "print(\"Classes:\", CLASS_NAMES)\n",
    "\n",
    "#tf.data building\n",
    "def decode_and_resize(filename, label):\n",
    "    image = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "#data listing\n",
    "file_paths = []\n",
    "labels = []\n",
    "for i, cls in enumerate(CLASS_NAMES):\n",
    "    cls_dir = os.path.join(DATA_DIR, cls)\n",
    "    for fn in os.listdir(cls_dir):\n",
    "        if fn.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            file_paths.append(os.path.join(cls_dir, fn))\n",
    "            labels.append(i)\n",
    "file_paths = np.array(file_paths)\n",
    "labels = np.array(labels)\n",
    "\n",
    "#shuffle and split (80% train, 10% val, 10% test)\n",
    "rng = np.random.RandomState(SEED)\n",
    "perm = rng.permutation(len(file_paths))\n",
    "file_paths = file_paths[perm]\n",
    "labels = labels[perm]\n",
    "\n",
    "n = len(file_paths)\n",
    "n_train = int(0.8 * n)\n",
    "n_val = int(0.1 * n)\n",
    "train_files, train_labels = file_paths[:n_train], labels[:n_train]\n",
    "val_files, val_labels = file_paths[n_train:n_train+n_val], labels[n_train:n_train+n_val]\n",
    "test_files, test_labels = file_paths[n_train+n_val:], labels[n_train+n_val:]\n",
    "\n",
    "def make_dataset(files, labels, training=False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((files, labels))\n",
    "    if training:\n",
    "        ds = ds.shuffle(buffer_size=len(files), seed=SEED)\n",
    "    ds = ds.map(decode_and_resize, num_parallel_calls=AUTOTUNE)\n",
    "    if training:\n",
    "        ds = ds.map(lambda x,y: (data_augment(x), y), num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7533d978",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#augementation\n",
    "data_augment = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.08),\n",
    "    tf.keras.layers.RandomZoom(0.08),\n",
    "    tf.keras.layers.RandomTranslation(0.05, 0.05),\n",
    "])\n",
    "\n",
    "train_ds = make_dataset(train_files, train_labels, training=True)\n",
    "val_ds = make_dataset(val_files, val_labels, training=False)\n",
    "test_ds = make_dataset(test_files, test_labels, training=False)\n",
    "\n",
    "#transfer learning with EfficientNetV2B0\n",
    "base_model = tf.keras.applications.EfficientNetV2B0(\n",
    "    include_top=False, input_shape=(*IMAGE_SIZE,3), weights=\"imagenet\"\n",
    ")\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = tf.keras.Input(shape=(*IMAGE_SIZE,3))\n",
    "x = inputs\n",
    "x = tf.keras.applications.efficientnet_v2.preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dropout(0.35)(x)\n",
    "x = tf.keras.layers.Dense(512, activation=\"swish\")(x)\n",
    "x = tf.keras.layers.Dropout(0.25)(x)\n",
    "outputs = tf.keras.layers.Dense(len(CLASS_NAMES), activation=\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.summary()\n",
    "\n",
    "#compile for head training\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "#callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(os.path.join(MODEL_DIR, \"best_head.h5\"),\n",
    "                                       save_best_only=True, monitor=\"val_accuracy\", mode=\"max\"),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-6),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "#train head\n",
    "history_head = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS_HEAD, callbacks=callbacks)\n",
    "\n",
    "#tuning\n",
    "base_model.trainable = True\n",
    "# freeze lower layers and unfreeze last N layers (tune N)\n",
    "N_UNFREEZE = 40\n",
    "for i, layer in enumerate(base_model.layers[:-N_UNFREEZE]):\n",
    "    layer.trainable = False\n",
    "for layer in base_model.layers[-N_UNFREEZE:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "#lower LR for fine-tuning\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "callbacks_fine = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(os.path.join(MODEL_DIR, \"best_finetuned.h5\"),\n",
    "                                       save_best_only=True, monitor=\"val_accuracy\", mode=\"max\"),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-7),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "history_ft = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS_FINETUNE, callbacks=callbacks_fine)\n",
    "\n",
    "#evaluate on test set\n",
    "model.save(os.path.join(MODEL_DIR, \"final_model_saved\"))\n",
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "\n",
    "# predict and show classification report\n",
    "y_pred_probs = model.predict(test_ds)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "#build true labels in same order as test_ds batches\n",
    "y_true = np.concatenate([y for x,y in test_ds], axis=0)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=CLASS_NAMES))\n",
    "\n",
    "# confusion matrix (optionally plot for top confusing classes)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "np.save(os.path.join(MODEL_DIR, \"confusion_matrix.npy\"), cm)\n",
    "\n",
    "#minimal plot of loss/acc\n",
    "def plot_hist(h, title):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(h.history.get(\"loss\", []), label=\"loss\")\n",
    "    plt.plot(h.history.get(\"val_loss\", []), label=\"val_loss\")\n",
    "    plt.title(title); plt.legend()\n",
    "\n",
    "plot_hist(history_head, \"head training\")\n",
    "plot_hist(history_ft, \"finetune training\")\n",
    "plt.show()\n",
    "\n",
    "#save mapping\n",
    "import json\n",
    "with open(os.path.join(MODEL_DIR, \"class_names.json\"), \"w\") as f:\n",
    "    json.dump(CLASS_NAMES, f)\n",
    "print(\"Saved model and artifacts to\", MODEL_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
